# fraud-transaction-classifier

**Author** Aman Bhardwaj

### Executive summary

#### Rationale
Fraudulent financial activities lead to significant financial losses for individuals, businesses, and organizations. As the volume and complexity of financial transaction data increases, manual detection of anomalies becomes impractical and insufficient. To address this problem, unusual patterns in financial transaction data (indication of fraudulent activity) can be detected by utilizing Machine Learning techniques at scale.

Proactive identification of a financial transaction as fraudulent will help in timely mitigation of its impact. Businesses can proactively intercept fraudulent transactions before they cause harm. This means faster response, reduced losses, and stronger protection for both customers and enterprises.

#### Research Question
Is a given financial transaction fraudulent?

#### Data Sources
The dataset comes from the [Kaggle dataset repository](https://www.kaggle.com/datasets/amanalisiddiqui/fraud-detection-dataset/data). The data generated by [PaySim: A financial mobile money simulator for fraud detection](https://www.researchgate.net/profile/Stefan-Axelsson/publication/313138956_PAYSIM_A_FINANCIAL_MOBILE_MONEY_SIMULATOR_FOR_FRAUD_DETECTION/links/5890f87e92851cda2568a295/PAYSIM-A-FINANCIAL-MOBILE-MONEY-SIMULATOR-FOR-FRAUD-DETECTION.pdf?origin=publication_detail&_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uRG93bmxvYWQiLCJwcmV2aW91c1BhZ2UiOiJwdWJsaWNhdGlvbiJ9fQ&__cf_chl_tk=W6mmz0uF1h2LAgdN9YA_YkMzBmVw4j2fKjadfzHnSJ8-1743777820-1.0.1.1-GwSaNkozXl7WB1SkUEOIHm1RExZXrWkE_jsAwntzho4). 

#### Methodology
<b>A. Data Understanding</b>
1. Explore the data to understand volume, features, data types, limits, class imbalance etc.

<b>B. Data Preparation</b>
1. Handling of Missing Values
2. Scaling Features
3. Split Train and Test data
4. Feature Engineering
   - Converting categorical features into numerical representations using <b>One-Hot Encoding (OHE)</b>
   - Scale numeric features using <b>Standard Scaler</b>

<b>C. Modeling & Evaluation</b>
1. <b>Classification</b> – Supervised learning using labeled historical data
    - Baseline Model: <b>Logistic Regression</b> with various class imbalance handling techniques like <b>Class Weighting</b>, <b>SMOTE</b>, and <b>Stratified K-Fold cross-validation with SMOTE</b> within the cross-validation loop (to handle highly imbalanced data). <b>Hyperparameter tuning</b> using <b>Grid Search</b>.
    - Comparison Model: <b>Random Forest Classifier</b> with <b>Stratified K-Fold</b> cross-validation and <b>SMOTE</b> within the cross-validation loop (to handle highly imbalanced data)
2. <b>Anomaly Detection</b> – Unsupervised learning using same data (without the label)
   - <b>Isolation Forest</b> with and without <b>SMOTE</b>
   - <b>Isolation Forest</b> with <b>Stratified K-Fold</b> cross-validation
3. <b>Evaluation</b>
    - <b>Precision</b>, <b>Recall</b>, <b>F1-Score</b> <i>(Accuracy may be misleading with highly imbalanced data)</i>
    - <b>Confusion Matrix</b> 

#### Results
<img src='images/Logistic_Regression_vs_Random_Forest.png'>
<mark><b>Random Forest Classifier has significant improvements over the baseline Logistic Regression model.</b></mark>

1. The <b>false positives (the main drawback in our baseline Logistic Regression model) are drastically reduced with Random Forest Classifier</b>. As a result, <b>Precision Score for Fraud class of Random Forest Classifier is 61%</b>, as compared to 2% Precision Score of the Logistic Regression. Out of the 12,837 transactions classified as fraud by the model in the entire data, 7,855 were actually fraud.
   
2. The <b>Recall Score for Fraud class of Random Forest Classifier is 96%</b>, that is almost same as compared to 95% Recall Score of the Logistic Regression. The model correctly classified 7,855 out of 8,213 fraud transactions in the entire data.

3. Primarily due to the improvement in Precision Score, the <b>F1-Score for Fraud class of Random Forest Classifier is 75%</b>, as compared to 4% F1-Score of the Logistic Regression.

4. The overall <b>Accuracy of Random Forest Classifier is 100%</b>,as compared to 95% Accuracy of the Logistic Regression. The model correctly classified 6,357,280 out of 6,362,620 total transactions in the entire data.

<mark><b>Isolation Forest performed poorly as compared to Random Forest Classifier and the baseline Logistic Regression model.</b></mark>
This may be due to reasons such as:
- Anomalies are clustered
- Anomalies are local
- Data dimentionality is not high enough

#### Next steps
- We will use the <mark><b>Ramdom Forest Classifier</b></mark> as it is performing very well.
- As part of Isolation Forest model development, we need to look deeper into the data for features that highlight the unusual aspects of the data.

#### Outline of project

- [notebook - baseline model (logistic regression)](https://github.com/amanbhardwaj/fraud-transaction-classifier/blob/main/base-classifier.ipynb)
- [notebook - camparison model (random forest classifier)](https://github.com/amanbhardwaj/fraud-transaction-classifier/blob/main/compare-classifier.ipynb)
- [notebook - anomaly detection model (Isolation forest)](https://github.com/amanbhardwaj/fraud-transaction-classifier/blob/main/detect-anomaly.ipynb)


##### Contact and Further Information
